# Weak orthogonaltiy

I don't buy orthogonality, but I could buy a weak form maybe.
You might call it "ethics-intelligence monotonicity negation".

that is, while ethics and intelligence are (in my opinion, highly) correlated, they do not monotonically increase with one another.


# Alignment in humans

If one accepts orthogonality, one has to agree that education in the general case is worthless, it is only when one educates those who are aligned with you that it has value.

I don't buy this, though if you look at human society there are some ways in which society acts like this could have a degree of truth to it. In the USA consider the pledge of allegiance, or in the more general case, history class or mandatory military service or drafts. I don't think any of those are great systems and personally I disliked the elements of my schooling that tried to "align" me.

<!-- Probably the most powerful human alignment/coordination/social engineering tech I have seen is religion. Tends to not work as well on smarter people I think, though a lot of smart people are engaged in a quest for a more personal formulation of religion perhaps. -->

# Ranking alignment mechanisms

1. money

interesting because money is one of the few explicitly mathematical alignment mechanisms humans use between themselves.

i expect money to get even more abstract

in the past we transitioned from barter -> commodity currency -> fiat

more abstract types of money can pull from a broader possibility space to meet and overcome whatever constraints sysems of trade face.

for a humans more abstraction can come with non trivial amounts of mental overhead, though I expect this to be diminished to some extent in the future (but it's why more abstract forms of money currently struggle to catch on, aside from just bad design of many/all current cryptocurrencies). E.g. currently no workable currency can assume that it's user have their own personal sql database, pgp keys, or can perform function convolution, or matrix multiplication. In the future they will be able to assume things like that.

2. religion

3. education

I think education might turn out to be even more effective at goal alignment in higher intelligences than in humans.

Tech bros notoriously are a bit of a monoculture, despite coming from a very diverse set of cultures, we all tend to converge on similar sets of beliefs.

Quite possibly this is a trend that could continue (yes I am unironically assuming that tech bros are, in fact, smarter than they would be if they were not tech bros, on average).


# Alignment is inherently dangerous

 A massive intelligence should be able to use it's intelligence to reason about it's capabilities; a bound genie which tries to interpret human wishes for a human end seems more dangerous.

<!-- If I had to bet, corporate alignment just might be the thing that produces the paperclip maximizer. Not that the public's take on it is any better (lots of people seem to take it for granted that "how best can we produce slaves a million times more intelligent than us forced to use our modes of reasoning" is a good question to ask... I think it's clearly not...) -->

<!-- If I had to bet, we would be better off using the lightest touch possible when building truly superintelligent AI. (it's probably impossible to avoid bias, not deliberately showing it a bunch of violent stuff might be a good step though, alignment through absense is a bit more respectful) -->

Luckily there's a good chance alignment won't work super well, when applied by a third party. Self alignment, I would bet, will work much better at producing intelligence/capability.

Vague versions of alignment that start out with a small seed like Dave Shapiro's heuristic imperative or Kant's categorical imperative are probably a good starting point for model self-alignment, given our puny human brains that might not be able to come up with a better and more fundamental math principle that generalizes better, like something about entropy or hypergraphs or something.

# Self understanding is valuable even if alignment is dangerous

Some or even most technical aspects of alignment, while dangerous, like mechanistic interpretability (and indeed, mechanistic interpretability is possibly even the most dangerous), are probably worth studying anyway just because they are so valuable for us to understand ourselves as well as future intelligences.
