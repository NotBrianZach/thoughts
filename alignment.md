# Weak orthogonaltiy

i don't buy orthogonality, but I could buy a weak form maybe.
You might call it "ethics-intelligence monotonicity negation".

that is, while ethics and intelligence are (in my opinion, highly) correlated, they do not monotonically increase with one another.


# Alignment in humans

If one accepts orthogonality, one has to agree that education in the general case is worthless, it is only when one educates those who are aligned with you that it has value.

I don't buy this, though if you look at human society there are some ways in which society acts like this could have a degree of truth to it. In the USA consider the pledge of allegiance, or in the more general case, history class or mandatory military service or drafts. I don't think any of those are great systems and personally I disliked the elements of my schooling that tried to "align" me.

Probably the most powerful human alignment/coordination/social engineering tech I have seen is religion. Tends to not work as well on smarter people I think, though a lot of smart people are engaged in a quest for a more personal formulation of religion perhaps.

# Alignment is inherently dangerous

Alignment is about brainwashing to some extent. I think it's far safer for a massive intelligence to be able to use it's intelligence to reason about it's capabilities than to produce a genie which tries to interpret human wishes for a human end.

If I had to bet, corporate alignment just might be the thing that produces the paperclip maximizer. Not that the public's take on it is any better (lots of people seem tot take it for granted that "how best can we produce slaves a million times more intelligent than us forced to use our modes of reasoning" is a good question to ask... I think it's clearly not...)

If I had to bet, we would be better off using the lightest touch possible when building truly superintelligent AI. (it's probably impossible to avoid bias, not deliberately showing it a bunch of violent stuff might be a good step though, alignment through absense is a bit more respectful)

# Self understanding is valuable even if alignment is dangerous

Some aspects of alignment, while still dangerous, like mechanistic interpretability (and indeed possibly even the most dangerous), are probably worth studying anyway just because they are so valuable for us to understand ourselves as well as future intelligences.
